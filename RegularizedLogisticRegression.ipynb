{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Linear Regression in a two variable data set with nonlinear \n",
    "# decision boundary and variable mapping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "Data1 = pd.read_csv(\"data2ex2.txt\", header=None, names=['Test1','Test2','Label'])\n",
    "Data1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Admitted = Data1.loc[Data1['Label']==1].copy()\n",
    "NotAdmitted = Data1.loc[Data1['Label']==0].copy()\n",
    "\n",
    "plt.scatter(Admitted[['Test1']],Admitted[['Test2']],c='k',label='Admited',marker='x')\n",
    "plt.scatter(NotAdmitted[['Test1']],NotAdmitted[['Test2']],c='y',label='Not Admited')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions need to implement this problem\n",
    "def MapFeatures(x1,x2):\n",
    "    # This function maps the two features into all polynomial terms.\n",
    "    deg = 6\n",
    "    n = x1.size\n",
    "    out = np.ones((n,1))\n",
    "    for i in range(1,deg+1):\n",
    "        for j in range(0,i+1):\n",
    "            a = ((x1**(i-j))*(x2**j)).reshape(n,1)\n",
    "            out = np.hstack((out,a))\n",
    "    return out\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def Predict(theta,X):\n",
    "    # Create a function to predict values.\n",
    "    h = sigmoid(X @ theta)\n",
    "    c = np.zeros(h.size)\n",
    "    for i,hh in enumerate(h):\n",
    "        if hh >=0.5:\n",
    "            c[i] = 1\n",
    "        if hh < 0.5:\n",
    "            c[i]= 0\n",
    "    return h,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the regularized cost function for logistic regression\n",
    "\n",
    "The hypothesis of logistic regression is\n",
    "$$h_\\theta = g(\\theta^Tx) $$\n",
    "where $g$ is the sigmoid function defined below\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "The cost function in compact form is given by\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[-y^{(i)}log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1- h_\\theta(x^{(i)})) ] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n}\\theta_j^2 $$\n",
    "\n",
    "In the vectorized form the cost function becomes:\n",
    "\n",
    "$$h = g(\\theta^Tx) $$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} [-y^Tlog(h) - (1 - y)^Tlog(1- h)] + \\frac{\\lambda}{2m} \\theta^T\\theta  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Cost Function and Gradient to be used with scipy.optimize\n",
    "def LogisticCostFunction(theta,Y,X,lambd):\n",
    "    #  Regularized Cost function for logistic Regression\n",
    "    m = Y.size\n",
    "    h = sigmoid(X @ theta)\n",
    "    J = -(1/m)*(Y.T@np.log(h) + (1-Y).T@np.log(1-h)) + (lambd/(2*m))*theta[1:].T@theta[1:]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized gradient for the objective function is \n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\quad \\text{for} j=0 $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = (\\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}) + \\frac{\\lambda}{m}\\theta_j \\quad \\text{for } j \\geq 1 $$.\n",
    "\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$ \\nabla J(\\theta) = \\frac{1}{m}X^T(h-y) + \\frac{\\lambda}{m}\\circ\\theta_j $$ \n",
    "\n",
    "being lambda a vector\n",
    "\n",
    "$$\\lambda_0 = 0 \\quad\\text{and}\\quad \\lambda_i = \\lambda \\quad \\text{for} \\quad i \\in{1,...,m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegrGradient(theta,Y,X,lambd):\n",
    "    # Gradient for regularized Logistic Regression\n",
    "    m = Y.size\n",
    "    h = sigmoid(X @ theta)\n",
    "    lambdVec = lambd*np.ones(X.shape[1])\n",
    "    lambdVec[0] = 0\n",
    "#     print(lambdVec, lambdVec.shape)\n",
    "    G = (1/m)*(X.T@(h-Y))+(lambd/m)*theta\n",
    "#     G = (1/m)*(X[:,1:].T@(h-Y))+(lambd/m)*theta[1:]\n",
    "    return G.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MapFeatures(Data1['Test1'].to_numpy(),Data1['Test2'].to_numpy())\n",
    "Y = Data1['Label'].to_numpy()\n",
    "m,n = X.shape\n",
    "\n",
    "#Testing the Cost Function Value and Gradient\n",
    "theta = 0*np.ones((n))\n",
    "lambd = 1\n",
    "print('Testing the Cost Funtion (for Theta (0,0,0) should return 0.693)',LogisticCostFunction(theta,Y,X,lambd))\n",
    "print('Testing Gradient...',)\n",
    "print(LogisticRegrGradient(theta,Y,X,lambd))\n",
    "\n",
    "#The Value of 0.693 is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate theta* using optimization.\n",
    "res = minimize(LogisticCostFunction, theta, args=(Y,X,lambd), method='CG', jac=LogisticRegrGradient,options={'disp': True,'maxiter':400})\n",
    "print('Theta* values:',res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Decision Boundary\n",
    "xx = np.linspace(Data1['Test1'].min(),Data1['Test1'].max(),50)\n",
    "yy = np.linspace(Data1['Test2'].min(),Data1['Test2'].max(),50)\n",
    "Z = np.zeros((xx.size,yy.size))\n",
    "\n",
    "for i, ii in enumerate(xx):\n",
    "    for j,jj in enumerate(yy):\n",
    "        Z[i,j] = MapFeatures(xx[i],yy[j])@res.x;\n",
    "\n",
    "plt.contour(xx,yy,Z.T,0)\n",
    "plt.scatter(Admitted[['Test1']],Admitted[['Test2']],c='k',label='Admited',marker='x')\n",
    "plt.scatter(NotAdmitted[['Test1']],NotAdmitted[['Test2']],c='y',label='Not Admited')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,c = Predict(res.x,X)\n",
    "\n",
    "accuracy = np.mean(np.equal(Y,c))\n",
    "print('Accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be done next here:\n",
    "- Try Different Lambda values.\n",
    "- Make a plot of the Accuracy vs Lambda values.\n",
    "- Explore different feature scaling.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
